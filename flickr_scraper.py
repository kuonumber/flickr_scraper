# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

# Generated by Glenn Jocher (glenn.jocher@ultralytics.com) for https://github.com/ultralytics

import argparse
import os
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from functools import partial

# å˜—è©¦è¼‰å…¥ .env æª”æ¡ˆ
try:
    from dotenv import load_dotenv
    # è¼‰å…¥ .env æª”æ¡ˆ
    load_dotenv()
    print("âœ… å·²è¼‰å…¥ .env æª”æ¡ˆ")
except ImportError:
    print("âš ï¸  æœªå®‰è£ python-dotenvï¼Œç„¡æ³•è‡ªå‹•è¼‰å…¥ .env æª”æ¡ˆ")
    print("ğŸ’¡ è«‹åŸ·è¡Œï¼špip install python-dotenv")
    print("ğŸ’¡ æˆ–æ‰‹å‹•è¨­å®šç’°å¢ƒè®Šæ•¸")

from flickrapi import FlickrAPI
from loguru import logger

from utils.general import download_uri

# å¾ç’°å¢ƒè®Šæ•¸è®€å– Flickr API æ†‘è­‰
key = os.getenv("FLICKR_API_KEY")
secret = os.getenv("FLICKR_API_SECRET")

# è¨­å®š loguru æ—¥èªŒ
def setup_logging():
    """è¨­å®š loguru æ—¥èªŒé…ç½®"""
    # ç§»é™¤é è¨­çš„ console handler
    logger.remove()
    
    # æ·»åŠ  console handler (INFO ç´šåˆ¥)
    logger.add(
        lambda msg: print(msg, end=""),
        level="INFO",
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        colorize=True
    )
    
    # æ·»åŠ æª”æ¡ˆ handler (DEBUG ç´šåˆ¥ï¼ŒåŒ…å«æ›´å¤šè©³ç´°è³‡è¨Š)
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    logger.add(
        log_dir / "flickr_scraper_{time:YYYY-MM-DD}.log",
        level="DEBUG",
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
        rotation="1 day",
        retention="30 days",
        compression="zip",
        encoding="utf-8"
    )
    
    # æ·»åŠ éŒ¯èª¤æ—¥èªŒæª”æ¡ˆ
    logger.add(
        log_dir / "errors_{time:YYYY-MM-DD}.log",
        level="ERROR",
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
        rotation="1 day",
        retention="30 days",
        compression="zip",
        encoding="utf-8"
    )
    
    logger.info("ğŸš€ Flickr Scraper æ—¥èªŒç³»çµ±å·²å•Ÿå‹•")
    logger.info(f"ğŸ“ æ—¥èªŒç›®éŒ„: {log_dir.absolute()}")

# åˆå§‹åŒ–æ—¥èªŒ
setup_logging()

# Flickr ç…§ç‰‡å°ºå¯¸å°æ‡‰è¡¨
SIZE_MAPPING = {
    "square": "url_sq",      # 75x75
    "large_square": "url_q", # 150x150
    "thumbnail": "url_t",    # 100x67
    "small": "url_s",        # 240x160
    "small_320": "url_n",    # 320x213
    "medium": "url_m",       # 500x333
    "medium_640": "url_z",   # 640x427
    "medium_800": "url_c",   # 800x533
    "large": "url_l",        # 1024x683
    "large_1600": "url_h",   # 1600x1067
    "large_2048": "url_k",   # 2048x1365
    "original": "url_o"      # åŸå§‹å°ºå¯¸
}

def get_urls(search="honeybees on flowers", n=10, download=False, size="large", thread_id=0):
    """Fetch Flickr URLs for `search` term images, optionally downloading them; supports up to `n` images.
    
    Args:
        search (str): æœå°‹é—œéµå­—
        n (int): ç…§ç‰‡æ•¸é‡
        download (bool): æ˜¯å¦ä¸‹è¼‰ç…§ç‰‡
        size (str): ç…§ç‰‡å°ºå¯¸ï¼Œå¯é¸å€¼ï¼šsquare, large_square, thumbnail, small, small_320, 
                   medium, medium_640, medium_800, large, large_1600, large_2048, original
        thread_id (int): ç·šç¨‹ IDï¼Œç”¨æ–¼å¤šç·šç¨‹è­˜åˆ¥
    """
    t = time.time()
    
    # ç‚ºæ¯å€‹ç·šç¨‹å‰µå»ºç¨ç«‹çš„ FlickrAPI å¯¦ä¾‹
    flickr = FlickrAPI(key, secret)
    license = ()  # https://www.flickr.com/services/api/explore/?method=flickr.photos.licenses.getInfo
    
    # æ ¹æ“šé¸æ“‡çš„å°ºå¯¸æ±ºå®šè¦ç²å–çš„ extras åƒæ•¸
    if size not in SIZE_MAPPING:
        logger.warning(f"[ç·šç¨‹ {thread_id}] ä¸æ”¯æ´çš„å°ºå¯¸ '{size}'ï¼Œä½¿ç”¨ 'large' æ›¿ä»£")
        size = "large"
    
    size_key = SIZE_MAPPING[size]
    extras = f"url_o,{size_key}" if size != "original" else "url_o"
    
    logger.info(f"[ç·šç¨‹ {thread_id}] é–‹å§‹æœå°‹ï¼š{search}")
    
    photos = flickr.walk(
        text=search,  # http://www.flickr.com/services/api/flickr.photos.search.html
        extras=extras,
        per_page=500,  # 1-500
        license=license,
        sort="relevance",
    )

    if download:
        dir_path = Path.cwd() / "images" / search.replace(" ", "_")
        dir_path.mkdir(parents=True, exist_ok=True)

    urls = []
    empty_file_count = 0  # ç©ºæª”æ¡ˆè¨ˆæ•¸å™¨
    consecutive_empty = 0  # é€£çºŒç©ºæª”æ¡ˆè¨ˆæ•¸å™¨
    max_consecutive_empty = 3  # æœ€å¤§é€£çºŒç©ºæª”æ¡ˆæ•¸
    
    for i, photo in enumerate(photos):
        if i <= n:
            try:
                # å„ªå…ˆä½¿ç”¨é¸æ“‡çš„å°ºå¯¸ï¼Œå¦‚æœæ²’æœ‰å‰‡å›é€€åˆ°å…¶ä»–é¸é …
                url = None
                if size == "original":
                    url = photo.get("url_o")
                else:
                    url = photo.get(size_key)
                
                # å¦‚æœé¸æ“‡çš„å°ºå¯¸æ²’æœ‰ï¼Œå‰‡å˜—è©¦å…¶ä»–å°ºå¯¸ä½œç‚ºå‚™é¸
                if url is None:
                    fallback_sizes = ["url_l", "url_c", "url_z", "url_m", "url_n"]
                    for fallback in fallback_sizes:
                        url = photo.get(fallback)
                        if url:
                            break
                
                # æœ€å¾Œçš„å‚™é¸æ–¹æ¡ˆ
                if url is None:
                    url = f"https://farm{photo.get('farm')}.staticflickr.com/{photo.get('server')}/{photo.get('id')}_{photo.get('secret')}_b.jpg"

                if download:
                    # ä¸‹è¼‰ä¸¦æª¢æŸ¥æª”æ¡ˆå¤§å°
                    file_path = download_uri(url, dir_path)
                    
                    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©ºæˆ–éå°
                    if file_path and file_path.exists():
                        file_size = file_path.stat().st_size
                        if file_size < 1000:  # å°æ–¼ 1KB è¦–ç‚ºç©ºæª”æ¡ˆ
                            empty_file_count += 1
                            consecutive_empty += 1
                            logger.warning(f"[ç·šç¨‹ {thread_id}] æª¢æ¸¬åˆ°ç©ºæª”æ¡ˆ ({file_size} bytes): {file_path.name}")
                            
                            # åˆªé™¤ç©ºæª”æ¡ˆ
                            try:
                                file_path.unlink()
                                logger.info(f"[ç·šç¨‹ {thread_id}] å·²åˆªé™¤ç©ºæª”æ¡ˆ: {file_path.name}")
                            except Exception as e:
                                logger.error(f"[ç·šç¨‹ {thread_id}] åˆªé™¤ç©ºæª”æ¡ˆå¤±æ•—: {e}")
                            
                            # æª¢æŸ¥æ˜¯å¦é”åˆ°é€£çºŒç©ºæª”æ¡ˆé™åˆ¶
                            if consecutive_empty >= max_consecutive_empty:
                                logger.error(f"[ç·šç¨‹ {thread_id}] é€£çºŒ {consecutive_empty} å€‹ç©ºæª”æ¡ˆï¼Œå¯èƒ½å·²é” API é™åˆ¶")
                                logger.warning(f"[ç·šç¨‹ {thread_id}] å»ºè­°ï¼šç­‰å¾…ä¸€æ®µæ™‚é–“å¾Œé‡è©¦ï¼Œæˆ–æª¢æŸ¥ API ä½¿ç”¨é‡")
                                break
                        else:
                            consecutive_empty = 0  # é‡ç½®é€£çºŒç©ºæª”æ¡ˆè¨ˆæ•¸
                            logger.debug(f"[ç·šç¨‹ {thread_id}] æª”æ¡ˆæ­£å¸¸ ({file_size} bytes): {file_path.name}")
                    else:
                        consecutive_empty += 1
                        logger.error(f"[ç·šç¨‹ {thread_id}] ä¸‹è¼‰å¤±æ•—: {url}")

                urls.append(url)
                logger.info(f"[ç·šç¨‹ {thread_id}] {i}/{n} {url}")
                
                # æ·»åŠ å»¶é²é¿å… API é™åˆ¶
                time.sleep(0.1)
                
            except Exception as e:
                consecutive_empty += 1
                logger.error(f"[ç·šç¨‹ {thread_id}] è™•ç†ç…§ç‰‡ {i}/{n} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                # æª¢æŸ¥æ˜¯å¦ç‚º API é™åˆ¶éŒ¯èª¤
                if "limit" in str(e).lower() or "quota" in str(e).lower():
                    logger.error(f"[ç·šç¨‹ {thread_id}] æª¢æ¸¬åˆ° API é™åˆ¶éŒ¯èª¤ï¼Œåœæ­¢è™•ç†")
                    break

        else:
            elapsed_time = time.time() - t
            logger.info(f"[ç·šç¨‹ {thread_id}] å®Œæˆæœå°‹ '{search}' ({elapsed_time:.1f}s)")
            if download:
                logger.info(f"[ç·šç¨‹ {thread_id}] åœ–ç‰‡å·²å„²å­˜è‡³ {dir_path}")
            break
    
    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    if empty_file_count > 0:
        logger.warning(f"[ç·šç¨‹ {thread_id}] çµ±è¨ˆï¼šç¸½å…± {len(urls)} å€‹ URLï¼Œ{empty_file_count} å€‹ç©ºæª”æ¡ˆ")
        if consecutive_empty >= max_consecutive_empty:
            logger.warning(f"[ç·šç¨‹ {thread_id}] è­¦å‘Šï¼šå¯èƒ½å·²é” API é™åˆ¶ï¼Œå»ºè­°æª¢æŸ¥ä½¿ç”¨é‡")
    
    return {
        'search': search,
        'urls': urls,
        'count': len(urls),
        'elapsed_time': time.time() - t,
        'thread_id': thread_id,
        'empty_files': empty_file_count,
        'api_limit_reached': consecutive_empty >= max_consecutive_empty
    }

def process_search_keyword(args):
    """å–®å€‹é—œéµå­—è™•ç†å‡½æ•¸ï¼Œç”¨æ–¼å¤šç·šç¨‹"""
    search, n, download, size, thread_id = args
    return get_urls(search=search, n=n, download=download, size=size, thread_id=thread_id)

def run_multithread_search(search_terms, n, download, size, max_workers=None):
    """ä½¿ç”¨å¤šç·šç¨‹è™•ç†å¤šå€‹æœå°‹é—œéµå­—
    
    Args:
        search_terms (list): æœå°‹é—œéµå­—åˆ—è¡¨
        n (int): æ¯å€‹é—œéµå­—çš„ç…§ç‰‡æ•¸é‡
        download (bool): æ˜¯å¦ä¸‹è¼‰ç…§ç‰‡
        size (str): ç…§ç‰‡å°ºå¯¸
        max_workers (int): æœ€å¤§ç·šç¨‹æ•¸ï¼Œé è¨­ç‚ºé—œéµå­—æ•¸é‡æˆ– CPU æ ¸å¿ƒæ•¸ * 2
    """
    if not max_workers:
        # å°æ–¼ I/O å¯†é›†å‹ä»»å‹™ï¼Œç·šç¨‹æ•¸å¯ä»¥æ¯” CPU æ ¸å¿ƒæ•¸å¤š
        max_workers = min(len(search_terms), os.cpu_count() * 2)
    
    print(f"ğŸš€ å•Ÿå‹•å¤šç·šç¨‹æ¨¡å¼")
    print(f"ğŸ“Š æœå°‹é—œéµå­—æ•¸é‡: {len(search_terms)}")
    print(f"âš™ï¸  æœ€å¤§ç·šç¨‹æ•¸: {max_workers}")
    print(f"ğŸ–¥ï¸  CPU æ ¸å¿ƒæ•¸: {os.cpu_count()}")
    print(f"ğŸ’¡ ç·šç¨‹æ•¸å»ºè­°: CPUæ ¸å¿ƒæ•¸ Ã— 2 (é©åˆ I/O å¯†é›†å‹ä»»å‹™)")
    print("=" * 50)
    
    # æº–å‚™ç·šç¨‹åƒæ•¸
    thread_args = [(search, n, download, size, i) for i, search in enumerate(search_terms)]
    
    # ä½¿ç”¨ç·šç¨‹æ± åŸ·è¡Œ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        future_to_search = {executor.submit(process_search_keyword, args): args[0] for args in thread_args}
        
        # æ”¶é›†çµæœ
        results = []
        for future in as_completed(future_to_search):
            search_term = future_to_search[future]
            try:
                result = future.result()
                results.append(result)
                print(f"âœ… å®Œæˆæœå°‹: {search_term}")
            except Exception as exc:
                print(f"âŒ æœå°‹ {search_term} æ™‚ç™¼ç”Ÿç•°å¸¸: {exc}")
    
    # æŒ‰ç·šç¨‹ ID æ’åºçµæœ
    results.sort(key=lambda x: x['thread_id'])
    
    # é¡¯ç¤ºçµæœæ‘˜è¦
    print("\n" + "=" * 50)
    print("ğŸ“‹ å¤šç·šç¨‹è™•ç†çµæœæ‘˜è¦")
    print("=" * 50)
    
    total_photos = 0
    total_time = 0
    total_empty_files = 0
    api_limit_reached_count = 0
    
    for result in results:
        print(f"ğŸ” '{result['search']}': {result['count']} å¼µç…§ç‰‡ ({result['elapsed_time']:.1f}s)")
        if result.get('empty_files', 0) > 0:
            print(f"   âš ï¸  ç©ºæª”æ¡ˆ: {result['empty_files']} å€‹")
        if result.get('api_limit_reached', False):
            print(f"   ğŸš« API é™åˆ¶: å¯èƒ½å·²é”ä¸Šé™")
            api_limit_reached_count += 1
        
        total_photos += result['count']
        total_empty_files += result.get('empty_files', 0)
        total_time = max(total_time, result['elapsed_time'])
    
    print(f"\nğŸ¯ ç¸½è¨ˆ: {total_photos} å¼µç…§ç‰‡")
    print(f"â±ï¸  ç¸½è€—æ™‚: {total_time:.1f}s")
    print(f"ğŸš€ æ•ˆç‡æå‡: {len(search_terms) * total_time / max(total_time, 1):.1f}x")
    
    if total_empty_files > 0:
        print(f"âš ï¸  ç©ºæª”æ¡ˆç¸½æ•¸: {total_empty_files}")
    
    if api_limit_reached_count > 0:
        print(f"ğŸš« API é™åˆ¶è­¦å‘Š: {api_limit_reached_count} å€‹é€²ç¨‹å¯èƒ½å·²é”ä¸Šé™")
        print("\n" + "=" * 60)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--search", nargs="+", default=["honeybees on flowers"], help="flickr search term")
    parser.add_argument("--n", type=int, default=10, help="number of images")
    parser.add_argument("--download", action="store_true", help="download images")
    parser.add_argument("--size", type=str, default="large", 
                       choices=list(SIZE_MAPPING.keys()),
                       help="photo size: square(75x75), large_square(150x150), thumbnail(100x67), "
                            "small(240x160), small_320(320x213), medium(500x333), medium_640(640x427), "
                            "medium_800(800x533), large(1024x683), large_1600(1600x1067), "
                            "large_2048(2048x1365), original")
    parser.add_argument("--max-workers", type=int, default=None, 
                       help="æœ€å¤§ç·šç¨‹æ•¸ï¼Œé è¨­ç‚ºé—œéµå­—æ•¸é‡æˆ– CPU æ ¸å¿ƒæ•¸ * 2")
    opt = parser.parse_args()

    print(f"ğŸ” æœå°‹é—œéµå­—: {opt.search}")
    print(f"ğŸ“ ç…§ç‰‡å°ºå¯¸: {opt.size}")
    print(f"ğŸ“Š æ¯å€‹é—œéµå­—ç…§ç‰‡æ•¸é‡: {opt.n}")
    print(f"ğŸ’¾ ä¸‹è¼‰æ¨¡å¼: {'é–‹å•Ÿ' if opt.download else 'é—œé–‰'}")
    
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸æ˜¯å¦è¨­å®š
    if not key or not secret:
        print("âŒ éŒ¯èª¤ï¼šè«‹è¨­å®šç’°å¢ƒè®Šæ•¸ FLICKR_API_KEY å’Œ FLICKR_API_SECRET")
        print("ğŸ’¡ è¨­å®šæ–¹æ³•ï¼š")
        print("   export FLICKR_API_KEY='your_api_key_here'")
        print("   export FLICKR_API_SECRET='your_api_secret_here'")
        print("   ğŸ”— ç”³è«‹ API é‡‘é‘°ï¼šhttps://www.flickr.com/services/apps/create/apply")
        exit(1)
    
    print("âœ… Flickr API æ†‘è­‰å·²è¼‰å…¥")
    print()

    # æ ¹æ“šé—œéµå­—æ•¸é‡æ±ºå®šæ˜¯å¦ä½¿ç”¨å¤šç·šç¨‹
    if len(opt.search) > 1:
        # å¤šå€‹é—œéµå­—ï¼Œä½¿ç”¨å¤šç·šç¨‹
        run_multithread_search(
            search_terms=opt.search,
            n=opt.n,
            download=opt.download,
            size=opt.size,
            max_workers=opt.max_workers
        )
    else:
        # å–®å€‹é—œéµå­—ï¼Œä½¿ç”¨å–®ç·šç¨‹
        print("ğŸ”„ å–®ç·šç¨‹æ¨¡å¼")
        result = get_urls(
            search=opt.search[0], 
            n=opt.n, 
            download=opt.download, 
            size=opt.size,
            thread_id=0
        )
        print(f"\nâœ… å®Œæˆ: {result['count']} å¼µç…§ç‰‡ ({result['elapsed_time']:.1f}s)")
