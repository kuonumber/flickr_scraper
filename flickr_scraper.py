# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

# Generated by Glenn Jocher (glenn.jocher@ultralytics.com) for https://github.com/ultralytics

import argparse
import os
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from functools import partial

# å˜—è©¦è¼‰å…¥ .env æª”æ¡ˆ
try:
    from dotenv import load_dotenv
    # è¼‰å…¥ .env æª”æ¡ˆ
    load_dotenv()
    print("âœ… å·²è¼‰å…¥ .env æª”æ¡ˆ")
except ImportError:
    print("âš ï¸  æœªå®‰è£ python-dotenvï¼Œç„¡æ³•è‡ªå‹•è¼‰å…¥ .env æª”æ¡ˆ")
    print("ğŸ’¡ è«‹åŸ·è¡Œï¼špip install python-dotenv")
    print("ğŸ’¡ æˆ–æ‰‹å‹•è¨­å®šç’°å¢ƒè®Šæ•¸")

from flickrapi import FlickrAPI
from loguru import logger

from utils.general import download_uri

# å¾ç’°å¢ƒè®Šæ•¸è®€å– Flickr API æ†‘è­‰
key = os.getenv("FLICKR_API_KEY")
secret = os.getenv("FLICKR_API_SECRET")

def get_user_albums(user_id=None, owner_name=None):
    """ç²å–ç”¨æˆ¶çš„æ‰€æœ‰ç›¸ç°¿åˆ—è¡¨
    
    Args:
        user_id (str): Flickr ç”¨æˆ¶ ID
        owner_name (str): Flickr ç”¨æˆ¶å
        
    Returns:
        list: ç›¸ç°¿åˆ—è¡¨ï¼Œæ¯å€‹ç›¸ç°¿åŒ…å« id, title, description, count_photos
    """
    try:
        flickr = FlickrAPI(key, secret)
        
        # å¦‚æœåªæœ‰ç”¨æˆ¶åï¼Œéœ€è¦å…ˆç²å–ç”¨æˆ¶ ID
        if owner_name and not user_id:
            try:
                # æœå°‹ç”¨æˆ¶
                user_search = flickr.people.findByUsername(username=owner_name)
                if user_search and user_search.find('user') is not None:
                    user_id = user_search.find('user').get('id')
                    logger.info(f"æ‰¾åˆ°ç”¨æˆ¶ '{owner_name}' çš„ ID: {user_id}")
                else:
                    logger.error(f"æ‰¾ä¸åˆ°ç”¨æˆ¶å: {owner_name}")
                    return []
            except Exception as e:
                logger.error(f"æœå°‹ç”¨æˆ¶ '{owner_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                return []
        
        if not user_id:
            logger.error("éœ€è¦æä¾› user_id æˆ– owner_name")
            return []
        
        # ç²å–ç”¨æˆ¶çš„ç›¸ç°¿åˆ—è¡¨
        try:
            albums = flickr.photosets.getList(user_id=user_id)
            
            if not albums or not albums.find('photosets'):
                logger.warning(f"ç”¨æˆ¶ {user_id} æ²’æœ‰ç›¸ç°¿æˆ–ç›¸ç°¿åˆ—è¡¨ç‚ºç©º")
                return []
            
            photosets = albums.find('photosets')
            if photosets is None:
                logger.warning(f"ç”¨æˆ¶ {user_id} æ²’æœ‰ç›¸ç°¿")
                return []
            
            album_list = []
            for photoset in photosets.findall('photoset'):
                title_elem = photoset.find('title')
                description_elem = photoset.find('description')
                
                album_info = {
                    'id': photoset.get('id'),
                    'title': title_elem.text if title_elem is not None else 'Untitled',
                    'description': description_elem.text if description_elem is not None else '',
                    'count_photos': int(photoset.get('count_photos', 0)),
                    'count_videos': int(photoset.get('count_videos', 0))
                }
                album_list.append(album_info)
                logger.info(f"ç›¸ç°¿: {album_info['title']} (ID: {album_info['id']}, ç…§ç‰‡: {album_info['count_photos']})")
            
            return album_list
            
        except Exception as e:
            logger.error(f"ç²å–ç”¨æˆ¶ {user_id} çš„ç›¸ç°¿åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
        
    except Exception as e:
        logger.error(f"ç²å–ç›¸ç°¿åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def get_album_photos(album_id, n=10, size="large", user_id=None, owner_name=None):
    """ç²å–ç›¸ç°¿ä¸­çš„æ‰€æœ‰ç…§ç‰‡
    
    Args:
        album_id (str): ç›¸ç°¿ ID
        n (int): ç…§ç‰‡æ•¸é‡é™åˆ¶
        size (str): ç…§ç‰‡å°ºå¯¸
        user_id (str): ç”¨æˆ¶ IDï¼ˆå¯é¸ï¼‰
        owner_name (str): ç”¨æˆ¶åï¼ˆå¯é¸ï¼‰
        
    Returns:
        dict: åŒ…å«ç›¸ç°¿æ¨™é¡Œå’Œç…§ç‰‡ URL åˆ—è¡¨çš„å­—å…¸
    """
    try:
        flickr = FlickrAPI(key, secret)
        
        # æ ¹æ“šé¸æ“‡çš„å°ºå¯¸æ±ºå®šè¦ç²å–çš„ extras åƒæ•¸
        if size not in SIZE_MAPPING:
            logger.warning(f"ä¸æ”¯æ´çš„å°ºå¯¸ '{size}'ï¼Œä½¿ç”¨ 'large' æ›¿ä»£")
            size = "large"
        
        size_key = SIZE_MAPPING[size]
        extras = f"url_o,{size_key}" if size != "original" else "url_o"
        
        # å…ˆæª¢æŸ¥ç›¸ç°¿æ˜¯å¦å­˜åœ¨ä¸¦ç²å–ç›¸ç°¿è³‡è¨Š
        try:
            photoset_info = flickr.photosets.getInfo(photoset_id=album_id)
            if not photoset_info:
                logger.error(f"ç›¸ç°¿ {album_id} ä¸å­˜åœ¨")
                return []
            
            photoset = photoset_info.find('photoset')
            if not photoset:
                logger.error(f"ç„¡æ³•ç²å–ç›¸ç°¿ {album_id} çš„è³‡è¨Š")
                return []
            
            # ç²å–ç›¸ç°¿æ¨™é¡Œå’Œæ“æœ‰è€… ID
            title_elem = photoset.find('title')
            title = title_elem.text if title_elem is not None else f"Album {album_id}"
            
            owner_elem = photoset.find('owner')
            album_owner_id = owner_elem.get('nsid') if owner_elem is not None else None
            
            logger.info(f"æ‰¾åˆ°ç›¸ç°¿: {title}")
            if album_owner_id:
                logger.info(f"ç›¸ç°¿æ“æœ‰è€… ID: {album_owner_id}")
            
            # å¦‚æœæ²’æœ‰æä¾›ç”¨æˆ¶ IDï¼Œä½¿ç”¨ç›¸ç°¿çš„æ“æœ‰è€… ID
            if not user_id and not owner_name and album_owner_id:
                user_id = album_owner_id
                logger.info(f"è‡ªå‹•ä½¿ç”¨ç›¸ç°¿æ“æœ‰è€… ID: {user_id}")
            
        except Exception as e:
            logger.error(f"ç›¸ç°¿ {album_id} ä¸å­˜åœ¨æˆ–ç„¡æ³•è¨ªå•: {e}")
            return {"title": f"Album_{album_id}", "urls": []}
        
        # ç²å–ç›¸ç°¿ä¸­çš„ç…§ç‰‡
        photos = flickr.photosets.getPhotos(
            photoset_id=album_id,
            extras=extras,
            per_page=500
        )
        
        if not photos or not photos.find('photoset'):
            logger.warning(f"ç›¸ç°¿ {album_id} ä¸­æ²’æœ‰ç…§ç‰‡")
            return {"title": title, "urls": []}
        
        photoset = photos.find('photoset')
        photo_list = photoset.findall('photo')
        
        if not photo_list:
            logger.warning(f"ç›¸ç°¿ {album_id} ä¸­æ²’æœ‰ç…§ç‰‡")
            return {"title": title, "urls": []}
        
        urls = []
        for i, photo in enumerate(photo_list):
            if i >= n:
                break
                
            # ç²å–ç…§ç‰‡ URL
            url = None
            if size == "original":
                url = photo.get("url_o")
            else:
                url = photo.get(size_key)
            
            # å¦‚æœé¸æ“‡çš„å°ºå¯¸æ²’æœ‰ï¼Œå‰‡å˜—è©¦å…¶ä»–å°ºå¯¸ä½œç‚ºå‚™é¸
            if url is None:
                fallback_sizes = ["url_l", "url_c", "url_z", "url_m", "url_n"]
                for fallback in fallback_sizes:
                    url = photo.get(fallback)
                    if url:
                        break
            
            # æœ€å¾Œçš„å‚™é¸æ–¹æ¡ˆ
            if url is None:
                url = f"https://farm{photo.get('farm')}.staticflickr.com/{photo.get('server')}/{photo.get('id')}_{photo.get('secret')}_b.jpg"
            
            urls.append(url)
        
        logger.info(f"å¾ç›¸ç°¿ {title} (ID: {album_id}) ç²å–åˆ° {len(urls)} å¼µç…§ç‰‡")
        return {"title": title, "urls": urls}
        
    except Exception as e:
        logger.error(f"ç²å–ç›¸ç°¿ç…§ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {"title": f"Album_{album_id}", "urls": []}

# è¨­å®š loguru æ—¥èªŒ
def setup_logging():
    """è¨­å®š loguru æ—¥èªŒé…ç½®"""
    # ç§»é™¤é è¨­çš„ console handler
    logger.remove()
    
    # æ·»åŠ  console handler (INFO ç´šåˆ¥)
    logger.add(
        lambda msg: print(msg, end=""),
        level="INFO",
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        colorize=True
    )
    
    # æ·»åŠ æª”æ¡ˆ handler (DEBUG ç´šåˆ¥ï¼ŒåŒ…å«æ›´å¤šè©³ç´°è³‡è¨Š)
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    logger.add(
        log_dir / "flickr_scraper_{time:YYYY-MM-DD}.log",
        level="DEBUG",
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
        rotation="1 day",
        retention="30 days",
        compression="zip",
        encoding="utf-8"
    )
    
    # æ·»åŠ éŒ¯èª¤æ—¥èªŒæª”æ¡ˆ
    logger.add(
        log_dir / "errors_{time:YYYY-MM-DD}.log",
        level="ERROR",
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
        rotation="1 day",
        retention="30 days",
        compression="zip",
        encoding="utf-8"
    )
    
    logger.info("ğŸš€ Flickr Scraper æ—¥èªŒç³»çµ±å·²å•Ÿå‹•")
    logger.info(f"ğŸ“ æ—¥èªŒç›®éŒ„: {log_dir.absolute()}")

# åˆå§‹åŒ–æ—¥èªŒ
setup_logging()

# Flickr ç…§ç‰‡å°ºå¯¸å°æ‡‰è¡¨
SIZE_MAPPING = {
    "square": "url_sq",      # 75x75
    "large_square": "url_q", # 150x150
    "thumbnail": "url_t",    # 100x67
    "small": "url_s",        # 240x160
    "small_320": "url_n",    # 320x213
    "medium": "url_m",       # 500x333
    "medium_640": "url_z",   # 640x427
    "medium_800": "url_c",   # 800x533
    "large": "url_l",        # 1024x683
    "large_1600": "url_h",   # 1600x1067
    "large_2048": "url_k",   # 2048x1365
    "original": "url_o"      # åŸå§‹å°ºå¯¸
}

def get_urls(search="honeybees on flowers", n=10, download=False, size="large", thread_id=0, output_dir="images", user_id=None, owner_name=None, all_albums=False, album_id=None):
    """Fetch Flickr URLs for `search` term images, optionally downloading them; supports up to `n` images.
    
    Args:
        search (str): æœå°‹é—œéµå­—
        n (int): ç…§ç‰‡æ•¸é‡
        download (bool): æ˜¯å¦ä¸‹è¼‰ç…§ç‰‡
        size (str): ç…§ç‰‡å°ºå¯¸
        thread_id (int): ç·šç¨‹ ID
        output_dir (str): è¼¸å‡ºç›®éŒ„è·¯å¾‘
        user_id (str): Flickr ç”¨æˆ¶ IDï¼ˆå¯é¸ï¼‰
        owner_name (str): Flickr ç”¨æˆ¶åï¼ˆå¯é¸ï¼‰
        all_albums (bool): æ˜¯å¦æœå°‹æ‰€æœ‰ç›¸ç°¿ï¼ˆç•¶é–å®šç”¨æˆ¶æ™‚ï¼‰
        album_id (str): ç›¸ç°¿ IDï¼ˆå¯é¸ï¼Œå„ªå…ˆæ–¼æœå°‹é—œéµå­—ï¼‰
    """
    t = time.time()
    
    # ç‚ºæ¯å€‹ç·šç¨‹å‰µå»ºç¨ç«‹çš„ FlickrAPI å¯¦ä¾‹
    flickr = FlickrAPI(key, secret)
    license = ()  # https://www.flickr.com/services/api/explore/?method=flickr.photos.licenses.getInfo
    
    # æ ¹æ“šé¸æ“‡çš„å°ºå¯¸æ±ºå®šè¦ç²å–çš„ extras åƒæ•¸
    if size not in SIZE_MAPPING:
        logger.warning(f"[ç·šç¨‹ {thread_id}] ä¸æ”¯æ´çš„å°ºå¯¸ '{size}'ï¼Œä½¿ç”¨ 'large' æ›¿ä»£")
        size = "large"
    
    size_key = SIZE_MAPPING[size]
    extras = f"url_o,{size_key}" if size != "original" else "url_o"
    
    logger.info(f"[ç·šç¨‹ {thread_id}] é–‹å§‹æœå°‹ï¼š{search}")
    
    # æº–å‚™æœå°‹åƒæ•¸
    search_params = {
        "extras": extras,
        "per_page": 500,  # 1-500
        "license": license,
        "sort": "relevance",
    }
    
    # æ·»åŠ å‰µä½œè€…é–å®šåƒæ•¸
    if user_id:
        search_params["user_id"] = user_id
        logger.info(f"[ç·šç¨‹ {thread_id}] é–å®šç”¨æˆ¶ ID: {user_id}")
    elif owner_name:
        search_params["owner_name"] = owner_name
        logger.info(f"[ç·šç¨‹ {thread_id}] é–å®šç”¨æˆ¶å: {owner_name}")
    
    # æ ¹æ“šæ˜¯å¦æŒ‡å®šç›¸ç°¿ ID ä¾†æ±ºå®šæœå°‹æ–¹å¼
    if album_id:
        # ç›¸ç°¿æ¨¡å¼ï¼šç›´æ¥å¾ç›¸ç°¿ç²å–ç…§ç‰‡
        logger.info(f"[ç·šç¨‹ {thread_id}] ç›¸ç°¿æ¨¡å¼ï¼šå¾ç›¸ç°¿ ID {album_id} ç²å–ç…§ç‰‡")
        album_result = get_album_photos(album_id, n, size, user_id, owner_name)
        
        # æª¢æŸ¥æ˜¯å¦æˆåŠŸç²å–åˆ°ç…§ç‰‡
        if not album_result or not album_result['urls']:
            logger.error(f"[ç·šç¨‹ {thread_id}] ç„¡æ³•å¾ç›¸ç°¿ {album_id} ç²å–ç…§ç‰‡")
            return {
                'search': f"album_{album_id}",
                'urls': [],
                'count': 0,
                'elapsed_time': time.time() - t,
                'thread_id': thread_id,
                'empty_files': 0,
                'api_limit_reached': False
            }
        
        urls = album_result['urls']
        album_title = album_result['title']
        
        if download:
            # å»ºç«‹ä¸‹è¼‰ç›®éŒ„ï¼Œä½¿ç”¨ç›¸ç°¿æ¨™é¡Œ
            # æ¸…ç†æ¨™é¡Œä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…æª”æ¡ˆç³»çµ±å•é¡Œ
            safe_title = "".join(c for c in album_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title.replace(' ', '_')
            if not safe_title:
                safe_title = f"Album_{album_id}"
            
            dir_path = Path.cwd() / output_dir / safe_title
            dir_path.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è¼‰ç…§ç‰‡
            downloaded_count = 0
            for i, url in enumerate(urls):
                if downloaded_count >= n:
                    break
                    
                try:
                    file_path = download_uri(url, dir_path)
                    if file_path and file_path.exists():
                        file_size = file_path.stat().st_size
                        if file_size >= 1000:
                            downloaded_count += 1
                            logger.info(f"[ç·šç¨‹ {thread_id}] ä¸‹è¼‰ç›¸ç°¿ç…§ç‰‡ {downloaded_count}/{n}: {file_path.name}")
                        else:
                            logger.warning(f"[ç·šç¨‹ {thread_id}] æª¢æ¸¬åˆ°ç©ºæª”æ¡ˆï¼Œå·²åˆªé™¤: {file_path.name}")
                            try:
                                file_path.unlink()
                            except:
                                pass
                    else:
                        logger.error(f"[ç·šç¨‹ {thread_id}] ä¸‹è¼‰å¤±æ•—: {url}")
                        
                    time.sleep(0.1)  # é¿å… API é™åˆ¶
                    
                except Exception as e:
                    logger.error(f"[ç·šç¨‹ {thread_id}] è™•ç†ç›¸ç°¿ç…§ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            logger.info(f"[ç·šç¨‹ {thread_id}] ç›¸ç°¿ç…§ç‰‡å·²å„²å­˜è‡³ {dir_path}")
        
        return {
            'search': f"album_{album_id}",
            'urls': urls,
            'count': len(urls),
            'elapsed_time': time.time() - t,
            'thread_id': thread_id,
            'empty_files': 0,
            'api_limit_reached': False
        }
    
    # æ ¹æ“šæ˜¯å¦å•Ÿç”¨æ‰€æœ‰ç›¸ç°¿ä¾†æ±ºå®šæœå°‹æ–¹å¼
    if (user_id or owner_name) and all_albums:
        # é–å®šç”¨æˆ¶ä¸”å•Ÿç”¨æ‰€æœ‰ç›¸ç°¿ï¼šæœå°‹è©²ç”¨æˆ¶çš„æ‰€æœ‰ç…§ç‰‡
        logger.info(f"[ç·šç¨‹ {thread_id}] å•Ÿç”¨æ‰€æœ‰ç›¸ç°¿æ¨¡å¼ï¼šæœå°‹ç”¨æˆ¶çš„æ‰€æœ‰ç…§ç‰‡")
        if search and search.strip():
            # å¦‚æœæœ‰é—œéµå­—ï¼Œåœ¨ç”¨æˆ¶æ‰€æœ‰ç›¸ç°¿ä¸­æœå°‹
            search_params["text"] = search
            logger.info(f"[ç·šç¨‹ {thread_id}] åœ¨ç”¨æˆ¶æ‰€æœ‰ç›¸ç°¿ä¸­æœå°‹é—œéµå­—ï¼š{search}")
        else:
            # å¦‚æœæ²’æœ‰é—œéµå­—ï¼Œä½¿ç”¨é€šç”¨é—œéµå­—æœå°‹ç”¨æˆ¶æ‰€æœ‰ç›¸ç°¿
            search_params["text"] = "*"
            logger.info(f"[ç·šç¨‹ {thread_id}] ä½¿ç”¨é€šç”¨é—œéµå­—æœå°‹ç”¨æˆ¶æ‰€æœ‰ç›¸ç°¿")
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨é—œéµå­—æœå°‹
        search_params["text"] = search
        logger.info(f"[ç·šç¨‹ {thread_id}] ä½¿ç”¨é—œéµå­—æœå°‹ï¼š{search}")
    
    photos = flickr.walk(**search_params)

    if download:
        dir_path = Path.cwd() / output_dir / search.replace(" ", "_")
        dir_path.mkdir(parents=True, exist_ok=True)

    urls = []
    empty_file_count = 0  # ç©ºæª”æ¡ˆè¨ˆæ•¸å™¨
    consecutive_empty = 0  # é€£çºŒç©ºæª”æ¡ˆè¨ˆæ•¸å™¨
    max_consecutive_empty = 3  # æœ€å¤§é€£çºŒç©ºæª”æ¡ˆæ•¸
    
    for i, photo in enumerate(photos):
        if i <= n:
            try:
                # å„ªå…ˆä½¿ç”¨é¸æ“‡çš„å°ºå¯¸ï¼Œå¦‚æœæ²’æœ‰å‰‡å›é€€åˆ°å…¶ä»–é¸é …
                url = None
                if size == "original":
                    url = photo.get("url_o")
                else:
                    url = photo.get(size_key)
                
                # å¦‚æœé¸æ“‡çš„å°ºå¯¸æ²’æœ‰ï¼Œå‰‡å˜—è©¦å…¶ä»–å°ºå¯¸ä½œç‚ºå‚™é¸
                if url is None:
                    fallback_sizes = ["url_l", "url_c", "url_z", "url_m", "url_n"]
                    for fallback in fallback_sizes:
                        url = photo.get(fallback)
                        if url:
                            break
                
                # æœ€å¾Œçš„å‚™é¸æ–¹æ¡ˆ
                if url is None:
                    url = f"https://farm{photo.get('farm')}.staticflickr.com/{photo.get('server')}/{photo.get('id')}_{photo.get('secret')}_b.jpg"

                if download:
                    # ä¸‹è¼‰ä¸¦æª¢æŸ¥æª”æ¡ˆå¤§å°
                    file_path = download_uri(url, dir_path)
                    
                    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©ºæˆ–éå°
                    if file_path and file_path.exists():
                        file_size = file_path.stat().st_size
                        if file_size < 1000:  # å°æ–¼ 1KB è¦–ç‚ºç©ºæª”æ¡ˆ
                            empty_file_count += 1
                            consecutive_empty += 1
                            logger.warning(f"[ç·šç¨‹ {thread_id}] æª¢æ¸¬åˆ°ç©ºæª”æ¡ˆ ({file_size} bytes): {file_path.name}")
                            
                            # åˆªé™¤ç©ºæª”æ¡ˆ
                            try:
                                file_path.unlink()
                                logger.info(f"[ç·šç¨‹ {thread_id}] å·²åˆªé™¤ç©ºæª”æ¡ˆ: {file_path.name}")
                            except Exception as e:
                                logger.error(f"[ç·šç¨‹ {thread_id}] åˆªé™¤ç©ºæª”æ¡ˆå¤±æ•—: {e}")
                            
                            # æª¢æŸ¥æ˜¯å¦é”åˆ°é€£çºŒç©ºæª”æ¡ˆé™åˆ¶
                            if consecutive_empty >= max_consecutive_empty:
                                logger.error(f"[ç·šç¨‹ {thread_id}] é€£çºŒ {consecutive_empty} å€‹ç©ºæª”æ¡ˆï¼Œå¯èƒ½å·²é” API é™åˆ¶")
                                logger.warning(f"[ç·šç¨‹ {thread_id}] å»ºè­°ï¼šç­‰å¾…ä¸€æ®µæ™‚é–“å¾Œé‡è©¦ï¼Œæˆ–æª¢æŸ¥ API ä½¿ç”¨é‡")
                                break
                        else:
                            consecutive_empty = 0  # é‡ç½®é€£çºŒç©ºæª”æ¡ˆè¨ˆæ•¸
                            logger.debug(f"[ç·šç¨‹ {thread_id}] æª”æ¡ˆæ­£å¸¸ ({file_size} bytes): {file_path.name}")
                    else:
                        consecutive_empty += 1
                        logger.error(f"[ç·šç¨‹ {thread_id}] ä¸‹è¼‰å¤±æ•—: {url}")

                urls.append(url)
                logger.info(f"[ç·šç¨‹ {thread_id}] {i}/{n} {url}")
                
                # æ·»åŠ å»¶é²é¿å… API é™åˆ¶
                time.sleep(0.1)
                
            except Exception as e:
                consecutive_empty += 1
                logger.error(f"[ç·šç¨‹ {thread_id}] è™•ç†ç…§ç‰‡ {i}/{n} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                # æª¢æŸ¥æ˜¯å¦ç‚º API é™åˆ¶éŒ¯èª¤
                if "limit" in str(e).lower() or "quota" in str(e).lower():
                    logger.error(f"[ç·šç¨‹ {thread_id}] æª¢æ¸¬åˆ° API é™åˆ¶éŒ¯èª¤ï¼Œåœæ­¢è™•ç†")
                    break

        else:
            elapsed_time = time.time() - t
            logger.info(f"[ç·šç¨‹ {thread_id}] å®Œæˆæœå°‹ '{search}' ({elapsed_time:.1f}s)")
            if download:
                logger.info(f"[ç·šç¨‹ {thread_id}] åœ–ç‰‡å·²å„²å­˜è‡³ {dir_path}")
            break
    
    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    if empty_file_count > 0:
        logger.warning(f"[ç·šç¨‹ {thread_id}] çµ±è¨ˆï¼šç¸½å…± {len(urls)} å€‹ URLï¼Œ{empty_file_count} å€‹ç©ºæª”æ¡ˆ")
        if consecutive_empty >= max_consecutive_empty:
            logger.warning(f"[ç·šç¨‹ {thread_id}] è­¦å‘Šï¼šå¯èƒ½å·²é” API é™åˆ¶ï¼Œå»ºè­°æª¢æŸ¥ä½¿ç”¨é‡")
    
    return {
        'search': search,
        'urls': urls,
        'count': len(urls),
        'elapsed_time': time.time() - t,
        'thread_id': thread_id,
        'empty_files': empty_file_count,
        'api_limit_reached': consecutive_empty >= max_consecutive_empty
    }

def process_search_keyword(args):
    """å–®å€‹é—œéµå­—è™•ç†å‡½æ•¸ï¼Œç”¨æ–¼å¤šç·šç¨‹"""
    search, n, download, size, thread_id, output_dir, user_id, owner_name, all_albums, album_id = args
    return get_urls(search=search, n=n, download=download, size=size, thread_id=thread_id, output_dir=output_dir, user_id=user_id, owner_name=owner_name, all_albums=all_albums, album_id=album_id)

def run_multithread_search(search_terms, n, download, size, max_workers=None, output_dir="images", user_id=None, owner_name=None, all_albums=False, album_id=None):
    """ä½¿ç”¨å¤šç·šç¨‹è™•ç†å¤šå€‹æœå°‹é—œéµå­—
    
    Args:
        search_terms (list): æœå°‹é—œéµå­—åˆ—è¡¨
        n (int): æ¯å€‹é—œéµå­—çš„ç…§ç‰‡æ•¸é‡
        download (bool): æ˜¯å¦ä¸‹è¼‰ç…§ç‰‡
        size (str): ç…§ç‰‡å°ºå¯¸
        max_workers (int): æœ€å¤§ç·šç¨‹æ•¸ï¼Œé è¨­ç‚ºé—œéµå­—æ•¸é‡æˆ– CPU æ ¸å¿ƒæ•¸ * 2
        output_dir (str): è¼¸å‡ºç›®éŒ„è·¯å¾‘
        user_id (str): Flickr ç”¨æˆ¶ IDï¼ˆå¯é¸ï¼‰
        owner_name (str): Flickr ç”¨æˆ¶åï¼ˆå¯é¸ï¼‰
        all_albums (bool): æ˜¯å¦æœå°‹æ‰€æœ‰ç›¸ç°¿ï¼ˆç•¶é–å®šç”¨æˆ¶æ™‚ï¼‰
        album_id (str): ç›¸ç°¿ IDï¼ˆå¯é¸ï¼‰
    """
    if not max_workers:
        # å°æ–¼ I/O å¯†é›†å‹ä»»å‹™ï¼Œç·šç¨‹æ•¸å¯ä»¥æ¯” CPU æ ¸å¿ƒæ•¸å¤š
        max_workers = min(len(search_terms), os.cpu_count() * 2)
    
    print(f"ğŸš€ å•Ÿå‹•å¤šç·šç¨‹æ¨¡å¼")
    print(f"ğŸ“Š æœå°‹é—œéµå­—æ•¸é‡: {len(search_terms)}")
    print(f"âš™ï¸  æœ€å¤§ç·šç¨‹æ•¸: {max_workers}")
    print(f"ğŸ–¥ï¸  CPU æ ¸å¿ƒæ•¸: {os.cpu_count()}")
    print(f"ğŸ’¡ ç·šç¨‹æ•¸å»ºè­°: CPUæ ¸å¿ƒæ•¸ Ã— 2 (é©åˆ I/O å¯†é›†å‹ä»»å‹™)")
    print("=" * 50)
    
    # æº–å‚™ç·šç¨‹åƒæ•¸
    thread_args = [(search, n, download, size, i, output_dir, user_id, owner_name, all_albums, album_id) for i, search in enumerate(search_terms)]
    
    # ä½¿ç”¨ç·šç¨‹æ± åŸ·è¡Œ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        future_to_search = {executor.submit(process_search_keyword, args): args[0] for args in thread_args}
        
        # æ”¶é›†çµæœ
        results = []
        for future in as_completed(future_to_search):
            search_term = future_to_search[future]
            try:
                result = future.result()
                results.append(result)
                print(f"âœ… å®Œæˆæœå°‹: {search_term}")
            except Exception as exc:
                print(f"âŒ æœå°‹ {search_term} æ™‚ç™¼ç”Ÿç•°å¸¸: {exc}")
    
    # æŒ‰ç·šç¨‹ ID æ’åºçµæœ
    results.sort(key=lambda x: x['thread_id'])
    
    # é¡¯ç¤ºçµæœæ‘˜è¦
    print("\n" + "=" * 50)
    print("ğŸ“‹ å¤šç·šç¨‹è™•ç†çµæœæ‘˜è¦")
    print("=" * 50)
    
    total_photos = 0
    total_time = 0
    total_empty_files = 0
    api_limit_reached_count = 0
    
    for result in results:
        print(f"ğŸ” '{result['search']}': {result['count']} å¼µç…§ç‰‡ ({result['elapsed_time']:.1f}s)")
        if result.get('empty_files', 0) > 0:
            print(f"   âš ï¸  ç©ºæª”æ¡ˆ: {result['empty_files']} å€‹")
        if result.get('api_limit_reached', False):
            print(f"   ğŸš« API é™åˆ¶: å¯èƒ½å·²é”ä¸Šé™")
            api_limit_reached_count += 1
        
        total_photos += result['count']
        total_empty_files += result.get('empty_files', 0)
        total_time = max(total_time, result['elapsed_time'])
    
    print(f"\nğŸ¯ ç¸½è¨ˆ: {total_photos} å¼µç…§ç‰‡")
    print(f"â±ï¸  ç¸½è€—æ™‚: {total_time:.1f}s")
    print(f"ğŸš€ æ•ˆç‡æå‡: {len(search_terms) * total_time / max(total_time, 1):.1f}x")
    
    if total_empty_files > 0:
        print(f"âš ï¸  ç©ºæª”æ¡ˆç¸½æ•¸: {total_empty_files}")
    
    if api_limit_reached_count > 0:
        print(f"ğŸš« API é™åˆ¶è­¦å‘Š: {api_limit_reached_count} å€‹é€²ç¨‹å¯èƒ½å·²é”ä¸Šé™")
        print("\n" + "=" * 60)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--search", nargs="+", default=["honeybees on flowers"], help="flickr search term")
    parser.add_argument("--n", type=int, default=10, help="number of images")
    parser.add_argument("--download", action="store_true", help="download images")
    parser.add_argument("--size", type=str, default="large", 
                       choices=list(SIZE_MAPPING.keys()),
                       help="photo size: square(75x75), large_square(150x150), thumbnail(100x67), "
                            "small(240x160), small_320(320x213), medium(500x333), medium_640(640x427), "
                            "medium_800(800x533), large(1024x683), large_1600(1600x1067), "
                            "large_2048(2048x1365), original")
    parser.add_argument("--max-workers", type=int, default=None, 
                       help="æœ€å¤§ç·šç¨‹æ•¸ï¼Œé è¨­ç‚ºé—œéµå­—æ•¸é‡æˆ– CPU æ ¸å¿ƒæ•¸ * 2")
    parser.add_argument("--output-dir", type=str, default="images", 
                       help="åœ–ç‰‡è¼¸å‡ºç›®éŒ„è·¯å¾‘ (é è¨­: images)")
    parser.add_argument("--user-id", type=str, default=None, 
                       help="Flickr ç”¨æˆ¶ IDï¼Œé–å®šç‰¹å®šå‰µä½œè€…")
    parser.add_argument("--owner-name", type=str, default=None, 
                       help="Flickr ç”¨æˆ¶åï¼Œé–å®šç‰¹å®šå‰µä½œè€…")
    parser.add_argument("--all-albums", action="store_true", 
                       help="ç•¶é–å®šç”¨æˆ¶æ™‚ï¼Œæœå°‹æ‰€æœ‰ç›¸ç°¿ï¼ˆä¸é™åˆ¶é—œéµå­—ï¼‰")
    parser.add_argument("--album-id", type=str, default=None, 
                       help="ç›¸ç°¿ IDï¼Œç›´æ¥å¾æŒ‡å®šç›¸ç°¿ä¸‹è¼‰ç…§ç‰‡")
    parser.add_argument("--list-albums", action="store_true", 
                       help="åˆ—å‡ºç”¨æˆ¶çš„æ‰€æœ‰ç›¸ç°¿ï¼ˆéœ€è¦ --user-id æˆ– --owner-nameï¼‰")
    parser.add_argument("--download-all-albums", action="store_true", 
                       help="ä¸‹è¼‰ç”¨æˆ¶çš„æ‰€æœ‰ç›¸ç°¿ï¼ˆéœ€è¦ --user-id æˆ– --owner-nameï¼‰")
    opt = parser.parse_args()

    print(f"ğŸ” æœå°‹é—œéµå­—: {opt.search}")
    print(f"ğŸ“ ç…§ç‰‡å°ºå¯¸: {opt.size}")
    print(f"ğŸ“Š æ¯å€‹é—œéµå­—ç…§ç‰‡æ•¸é‡: {opt.n}")
    print(f"ğŸ’¾ ä¸‹è¼‰æ¨¡å¼: {'é–‹å•Ÿ' if opt.download else 'é—œé–‰'}")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {opt.output_dir}")
    
    # é¡¯ç¤ºå‰µä½œè€…é–å®šè³‡è¨Š
    if opt.user_id:
        print(f"ğŸ‘¤ é–å®šç”¨æˆ¶ ID: {opt.user_id}")
    elif opt.owner_name:
        print(f"ğŸ‘¤ é–å®šç”¨æˆ¶å: {opt.owner_name}")
    else:
        print(f"ğŸ‘¤ å‰µä½œè€…é–å®š: ç„¡ï¼ˆæœå°‹æ‰€æœ‰ç”¨æˆ¶ï¼‰")
    
    # é¡¯ç¤ºç›¸ç°¿æœå°‹æ¨¡å¼
    if opt.album_id:
        print(f"ğŸ“š ç›¸ç°¿æœå°‹æ¨¡å¼: æŒ‡å®šç›¸ç°¿ (ID: {opt.album_id})")
    elif opt.all_albums and (opt.user_id or opt.owner_name):
        print(f"ğŸ“š ç›¸ç°¿æœå°‹æ¨¡å¼: æ‰€æœ‰ç›¸ç°¿")
    elif opt.user_id or opt.owner_name:
        print(f"ğŸ“š ç›¸ç°¿æœå°‹æ¨¡å¼: é—œéµå­—é™åˆ¶")
    else:
        print(f"ğŸ“š ç›¸ç°¿æœå°‹æ¨¡å¼: æ¨™æº–æœå°‹")
    
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸æ˜¯å¦è¨­å®š
    if not key or not secret:
        print("âŒ éŒ¯èª¤ï¼šè«‹è¨­å®šç’°å¢ƒè®Šæ•¸ FLICKR_API_KEY å’Œ FLICKR_API_SECRET")
        print("ğŸ’¡ è¨­å®šæ–¹æ³•ï¼š")
        print("   export FLICKR_API_KEY='your_api_key_here'")
        print("   export FLICKR_API_SECRET='your_api_secret_here'")
        print("   ğŸ”— ç”³è«‹ API é‡‘é‘°ï¼šhttps://www.flickr.com/services/apps/create/apply")
        exit(1)
    
    print("âœ… Flickr API æ†‘è­‰å·²è¼‰å…¥")
    print()

    # è™•ç†ç›¸ç°¿åˆ—è¡¨åŠŸèƒ½
    if opt.list_albums:
        if not opt.user_id and not opt.owner_name:
            print("âŒ éŒ¯èª¤ï¼š--list-albums éœ€è¦ --user-id æˆ– --owner-name åƒæ•¸")
            exit(1)
        
        print(f"ğŸ“‹ æ­£åœ¨ç²å–ç”¨æˆ¶ç›¸ç°¿åˆ—è¡¨...")
        albums = get_user_albums(opt.user_id, opt.owner_name)
        
        if albums:
            print(f"\nğŸ“š æ‰¾åˆ° {len(albums)} å€‹ç›¸ç°¿ï¼š")
            print("=" * 60)
            for i, album in enumerate(albums, 1):
                print(f"{i:2d}. {album['title']}")
                print(f"    ID: {album['id']}")
                print(f"    ç…§ç‰‡: {album['count_photos']} å¼µ")
                if album['count_videos'] > 0:
                    print(f"    å½±ç‰‡: {album['count_videos']} å€‹")
                if album['description']:
                    print(f"    æè¿°: {album['description'][:100]}{'...' if len(album['description']) > 100 else ''}")
                print()
        else:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ç›¸ç°¿æˆ–ç™¼ç”ŸéŒ¯èª¤")
        exit(0)

    # è™•ç†å…¨ç›¸ç°¿ä¸‹è¼‰åŠŸèƒ½
    if opt.download_all_albums:
        if not opt.user_id and not opt.owner_name:
            print("âŒ éŒ¯èª¤ï¼š--download-all-albums éœ€è¦ --user-id æˆ– --owner-name åƒæ•¸")
            exit(1)
        
        print(f"ğŸš€ æ­£åœ¨ä¸‹è¼‰ç”¨æˆ¶çš„æ‰€æœ‰ç›¸ç°¿...")
        albums = get_user_albums(opt.user_id, opt.owner_name)
        
        if not albums:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ç›¸ç°¿æˆ–ç™¼ç”ŸéŒ¯èª¤")
            exit(1)
        
        print(f"ğŸ“š æ‰¾åˆ° {len(albums)} å€‹ç›¸ç°¿ï¼Œé–‹å§‹å¤šç·šç¨‹ä¸‹è¼‰...")
        
        # ä½¿ç”¨å¤šç·šç¨‹ä¸‹è¼‰æ‰€æœ‰ç›¸ç°¿
        def download_album_worker(album_data):
            """ä¸‹è¼‰å–®å€‹ç›¸ç°¿çš„å·¥ä½œå‡½æ•¸"""
            album = album_data['album']
            thread_id = album_data['thread_id']
            
            logger.info(f"[ç·šç¨‹ {thread_id}] é–‹å§‹ä¸‹è¼‰ç›¸ç°¿: {album['title']} (ID: {album['id']})")
            
            result = get_urls(
                search="",  # ç©ºé—œéµå­—ï¼Œä½¿ç”¨ç›¸ç°¿ ID
                n=opt.n,
                download=opt.download,
                size=opt.size,
                thread_id=thread_id,
                output_dir=opt.output_dir,
                user_id=opt.user_id,
                owner_name=opt.owner_name,
                all_albums=False,  # ä¸æ˜¯å…¨ç›¸ç°¿æ¨¡å¼
                album_id=album['id']  # ç›´æ¥æŒ‡å®šç›¸ç°¿ ID
            )
            
            logger.info(f"[ç·šç¨‹ {thread_id}] ç›¸ç°¿ {album['title']} ä¸‹è¼‰å®Œæˆ: {result['count']} å¼µç…§ç‰‡")
            return result
        
        # æº–å‚™å·¥ä½œåƒæ•¸
        album_tasks = []
        for i, album in enumerate(albums, 1):
            album_tasks.append({
                'album': album,
                'thread_id': i
            })
        
        # ä½¿ç”¨ ThreadPoolExecutor é€²è¡Œå¤šç·šç¨‹ä¸‹è¼‰
        start_time = time.time()
        total_photos = 0
        
        with ThreadPoolExecutor(max_workers=opt.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_album = {
                executor.submit(download_album_worker, task): task 
                for task in album_tasks
            }
            
            # è™•ç†å®Œæˆçš„ä»»å‹™
            for future in as_completed(future_to_album):
                album_task = future_to_album[future]
                try:
                    result = future.result()
                    total_photos += result['count']
                    print(f"âœ… ç›¸ç°¿ {album_task['album']['title']} å®Œæˆ: {result['count']} å¼µç…§ç‰‡")
                except Exception as exc:
                    logger.error(f"ç›¸ç°¿ {album_task['album']['title']} ä¸‹è¼‰å¤±æ•—: {exc}")
                    print(f"âŒ ç›¸ç°¿ {album_task['album']['title']} ä¸‹è¼‰å¤±æ•—: {exc}")
        
        elapsed_time = time.time() - start_time
        print(f"\nğŸ¯ æ‰€æœ‰ç›¸ç°¿ä¸‹è¼‰å®Œæˆï¼")
        print(f"ğŸ“Š ç¸½è¨ˆ: {total_photos} å¼µç…§ç‰‡")
        print(f"â±ï¸  ç¸½è€—æ™‚: {elapsed_time:.1f}s")
        print(f"ğŸš€ ä½¿ç”¨ {opt.max_workers} å€‹ç·šç¨‹ä¸¦è¡Œè™•ç†")
        exit(0)

    # æ ¹æ“šé—œéµå­—æ•¸é‡æ±ºå®šæ˜¯å¦ä½¿ç”¨å¤šç·šç¨‹
    if len(opt.search) > 1:
        # å¤šå€‹é—œéµå­—ï¼Œä½¿ç”¨å¤šç·šç¨‹
        run_multithread_search(
            search_terms=opt.search,
            n=opt.n,
            download=opt.download,
            size=opt.size,
            max_workers=opt.max_workers,
            output_dir=opt.output_dir,
            user_id=opt.user_id,
            owner_name=opt.owner_name,
            all_albums=opt.all_albums
        )
    else:
        # å–®å€‹é—œéµå­—ï¼Œä½¿ç”¨å–®ç·šç¨‹
        print("ğŸ”„ å–®ç·šç¨‹æ¨¡å¼")
        result = get_urls(
            search=opt.search[0], 
            n=opt.n, 
            download=opt.download, 
            size=opt.size,
            thread_id=0,
            output_dir=opt.output_dir,
            user_id=opt.user_id,
            owner_name=opt.owner_name,
            all_albums=opt.all_albums,
            album_id=opt.album_id
        )
        print(f"\nâœ… å®Œæˆ: {result['count']} å¼µç…§ç‰‡ ({result['elapsed_time']:.1f}s)")
