# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license

# Generated by Glenn Jocher (glenn.jocher@ultralytics.com) for https://github.com/ultralytics

import argparse
import os
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from functools import partial

# 嘗試載入 .env 檔案
try:
    from dotenv import load_dotenv
    # 載入 .env 檔案
    load_dotenv()
    print("✅ 已載入 .env 檔案")
except ImportError:
    print("⚠️  未安裝 python-dotenv，無法自動載入 .env 檔案")
    print("💡 請執行：pip install python-dotenv")
    print("💡 或手動設定環境變數")

from flickrapi import FlickrAPI
from loguru import logger

from utils.general import download_uri

# 從環境變數讀取 Flickr API 憑證
key = os.getenv("FLICKR_API_KEY")
secret = os.getenv("FLICKR_API_SECRET")

# 設定 loguru 日誌
def setup_logging():
    """設定 loguru 日誌配置"""
    # 移除預設的 console handler
    logger.remove()
    
    # 添加 console handler (INFO 級別)
    logger.add(
        lambda msg: print(msg, end=""),
        level="INFO",
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        colorize=True
    )
    
    # 添加檔案 handler (DEBUG 級別，包含更多詳細資訊)
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    logger.add(
        log_dir / "flickr_scraper_{time:YYYY-MM-DD}.log",
        level="DEBUG",
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
        rotation="1 day",
        retention="30 days",
        compression="zip",
        encoding="utf-8"
    )
    
    # 添加錯誤日誌檔案
    logger.add(
        log_dir / "errors_{time:YYYY-MM-DD}.log",
        level="ERROR",
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
        rotation="1 day",
        retention="30 days",
        compression="zip",
        encoding="utf-8"
    )
    
    logger.info("🚀 Flickr Scraper 日誌系統已啟動")
    logger.info(f"📁 日誌目錄: {log_dir.absolute()}")

# 初始化日誌
setup_logging()

# Flickr 照片尺寸對應表
SIZE_MAPPING = {
    "square": "url_sq",      # 75x75
    "large_square": "url_q", # 150x150
    "thumbnail": "url_t",    # 100x67
    "small": "url_s",        # 240x160
    "small_320": "url_n",    # 320x213
    "medium": "url_m",       # 500x333
    "medium_640": "url_z",   # 640x427
    "medium_800": "url_c",   # 800x533
    "large": "url_l",        # 1024x683
    "large_1600": "url_h",   # 1600x1067
    "large_2048": "url_k",   # 2048x1365
    "original": "url_o"      # 原始尺寸
}

def get_urls(search="honeybees on flowers", n=10, download=False, size="large", thread_id=0):
    """Fetch Flickr URLs for `search` term images, optionally downloading them; supports up to `n` images.
    
    Args:
        search (str): 搜尋關鍵字
        n (int): 照片數量
        download (bool): 是否下載照片
        size (str): 照片尺寸，可選值：square, large_square, thumbnail, small, small_320, 
                   medium, medium_640, medium_800, large, large_1600, large_2048, original
        thread_id (int): 線程 ID，用於多線程識別
    """
    t = time.time()
    
    # 為每個線程創建獨立的 FlickrAPI 實例
    flickr = FlickrAPI(key, secret)
    license = ()  # https://www.flickr.com/services/api/explore/?method=flickr.photos.licenses.getInfo
    
    # 根據選擇的尺寸決定要獲取的 extras 參數
    if size not in SIZE_MAPPING:
        logger.warning(f"[線程 {thread_id}] 不支援的尺寸 '{size}'，使用 'large' 替代")
        size = "large"
    
    size_key = SIZE_MAPPING[size]
    extras = f"url_o,{size_key}" if size != "original" else "url_o"
    
    logger.info(f"[線程 {thread_id}] 開始搜尋：{search}")
    
    photos = flickr.walk(
        text=search,  # http://www.flickr.com/services/api/flickr.photos.search.html
        extras=extras,
        per_page=500,  # 1-500
        license=license,
        sort="relevance",
    )

    if download:
        dir_path = Path.cwd() / "images" / search.replace(" ", "_")
        dir_path.mkdir(parents=True, exist_ok=True)

    urls = []
    empty_file_count = 0  # 空檔案計數器
    consecutive_empty = 0  # 連續空檔案計數器
    max_consecutive_empty = 3  # 最大連續空檔案數
    
    for i, photo in enumerate(photos):
        if i <= n:
            try:
                # 優先使用選擇的尺寸，如果沒有則回退到其他選項
                url = None
                if size == "original":
                    url = photo.get("url_o")
                else:
                    url = photo.get(size_key)
                
                # 如果選擇的尺寸沒有，則嘗試其他尺寸作為備選
                if url is None:
                    fallback_sizes = ["url_l", "url_c", "url_z", "url_m", "url_n"]
                    for fallback in fallback_sizes:
                        url = photo.get(fallback)
                        if url:
                            break
                
                # 最後的備選方案
                if url is None:
                    url = f"https://farm{photo.get('farm')}.staticflickr.com/{photo.get('server')}/{photo.get('id')}_{photo.get('secret')}_b.jpg"

                if download:
                    # 下載並檢查檔案大小
                    file_path = download_uri(url, dir_path)
                    
                    # 檢查檔案是否為空或過小
                    if file_path and file_path.exists():
                        file_size = file_path.stat().st_size
                        if file_size < 1000:  # 小於 1KB 視為空檔案
                            empty_file_count += 1
                            consecutive_empty += 1
                            logger.warning(f"[線程 {thread_id}] 檢測到空檔案 ({file_size} bytes): {file_path.name}")
                            
                            # 刪除空檔案
                            try:
                                file_path.unlink()
                                logger.info(f"[線程 {thread_id}] 已刪除空檔案: {file_path.name}")
                            except Exception as e:
                                logger.error(f"[線程 {thread_id}] 刪除空檔案失敗: {e}")
                            
                            # 檢查是否達到連續空檔案限制
                            if consecutive_empty >= max_consecutive_empty:
                                logger.error(f"[線程 {thread_id}] 連續 {consecutive_empty} 個空檔案，可能已達 API 限制")
                                logger.warning(f"[線程 {thread_id}] 建議：等待一段時間後重試，或檢查 API 使用量")
                                break
                        else:
                            consecutive_empty = 0  # 重置連續空檔案計數
                            logger.debug(f"[線程 {thread_id}] 檔案正常 ({file_size} bytes): {file_path.name}")
                    else:
                        consecutive_empty += 1
                        logger.error(f"[線程 {thread_id}] 下載失敗: {url}")

                urls.append(url)
                logger.info(f"[線程 {thread_id}] {i}/{n} {url}")
                
                # 添加延遲避免 API 限制
                time.sleep(0.1)
                
            except Exception as e:
                consecutive_empty += 1
                logger.error(f"[線程 {thread_id}] 處理照片 {i}/{n} 時發生錯誤: {e}")
                
                # 檢查是否為 API 限制錯誤
                if "limit" in str(e).lower() or "quota" in str(e).lower():
                    logger.error(f"[線程 {thread_id}] 檢測到 API 限制錯誤，停止處理")
                    break

        else:
            elapsed_time = time.time() - t
            logger.info(f"[線程 {thread_id}] 完成搜尋 '{search}' ({elapsed_time:.1f}s)")
            if download:
                logger.info(f"[線程 {thread_id}] 圖片已儲存至 {dir_path}")
            break
    
    # 顯示統計資訊
    if empty_file_count > 0:
        logger.warning(f"[線程 {thread_id}] 統計：總共 {len(urls)} 個 URL，{empty_file_count} 個空檔案")
        if consecutive_empty >= max_consecutive_empty:
            logger.warning(f"[線程 {thread_id}] 警告：可能已達 API 限制，建議檢查使用量")
    
    return {
        'search': search,
        'urls': urls,
        'count': len(urls),
        'elapsed_time': time.time() - t,
        'thread_id': thread_id,
        'empty_files': empty_file_count,
        'api_limit_reached': consecutive_empty >= max_consecutive_empty
    }

def process_search_keyword(args):
    """單個關鍵字處理函數，用於多線程"""
    search, n, download, size, thread_id = args
    return get_urls(search=search, n=n, download=download, size=size, thread_id=thread_id)

def run_multithread_search(search_terms, n, download, size, max_workers=None):
    """使用多線程處理多個搜尋關鍵字
    
    Args:
        search_terms (list): 搜尋關鍵字列表
        n (int): 每個關鍵字的照片數量
        download (bool): 是否下載照片
        size (str): 照片尺寸
        max_workers (int): 最大線程數，預設為關鍵字數量或 CPU 核心數 * 2
    """
    if not max_workers:
        # 對於 I/O 密集型任務，線程數可以比 CPU 核心數多
        max_workers = min(len(search_terms), os.cpu_count() * 2)
    
    print(f"🚀 啟動多線程模式")
    print(f"📊 搜尋關鍵字數量: {len(search_terms)}")
    print(f"⚙️  最大線程數: {max_workers}")
    print(f"🖥️  CPU 核心數: {os.cpu_count()}")
    print(f"💡 線程數建議: CPU核心數 × 2 (適合 I/O 密集型任務)")
    print("=" * 50)
    
    # 準備線程參數
    thread_args = [(search, n, download, size, i) for i, search in enumerate(search_terms)]
    
    # 使用線程池執行
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # 提交所有任務
        future_to_search = {executor.submit(process_search_keyword, args): args[0] for args in thread_args}
        
        # 收集結果
        results = []
        for future in as_completed(future_to_search):
            search_term = future_to_search[future]
            try:
                result = future.result()
                results.append(result)
                print(f"✅ 完成搜尋: {search_term}")
            except Exception as exc:
                print(f"❌ 搜尋 {search_term} 時發生異常: {exc}")
    
    # 按線程 ID 排序結果
    results.sort(key=lambda x: x['thread_id'])
    
    # 顯示結果摘要
    print("\n" + "=" * 50)
    print("📋 多線程處理結果摘要")
    print("=" * 50)
    
    total_photos = 0
    total_time = 0
    total_empty_files = 0
    api_limit_reached_count = 0
    
    for result in results:
        print(f"🔍 '{result['search']}': {result['count']} 張照片 ({result['elapsed_time']:.1f}s)")
        if result.get('empty_files', 0) > 0:
            print(f"   ⚠️  空檔案: {result['empty_files']} 個")
        if result.get('api_limit_reached', False):
            print(f"   🚫 API 限制: 可能已達上限")
            api_limit_reached_count += 1
        
        total_photos += result['count']
        total_empty_files += result.get('empty_files', 0)
        total_time = max(total_time, result['elapsed_time'])
    
    print(f"\n🎯 總計: {total_photos} 張照片")
    print(f"⏱️  總耗時: {total_time:.1f}s")
    print(f"🚀 效率提升: {len(search_terms) * total_time / max(total_time, 1):.1f}x")
    
    if total_empty_files > 0:
        print(f"⚠️  空檔案總數: {total_empty_files}")
    
    if api_limit_reached_count > 0:
        print(f"🚫 API 限制警告: {api_limit_reached_count} 個進程可能已達上限")
        print("\n" + "=" * 60)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--search", nargs="+", default=["honeybees on flowers"], help="flickr search term")
    parser.add_argument("--n", type=int, default=10, help="number of images")
    parser.add_argument("--download", action="store_true", help="download images")
    parser.add_argument("--size", type=str, default="large", 
                       choices=list(SIZE_MAPPING.keys()),
                       help="photo size: square(75x75), large_square(150x150), thumbnail(100x67), "
                            "small(240x160), small_320(320x213), medium(500x333), medium_640(640x427), "
                            "medium_800(800x533), large(1024x683), large_1600(1600x1067), "
                            "large_2048(2048x1365), original")
    parser.add_argument("--max-workers", type=int, default=None, 
                       help="最大線程數，預設為關鍵字數量或 CPU 核心數 * 2")
    opt = parser.parse_args()

    print(f"🔍 搜尋關鍵字: {opt.search}")
    print(f"📏 照片尺寸: {opt.size}")
    print(f"📊 每個關鍵字照片數量: {opt.n}")
    print(f"💾 下載模式: {'開啟' if opt.download else '關閉'}")
    
    # 檢查環境變數是否設定
    if not key or not secret:
        print("❌ 錯誤：請設定環境變數 FLICKR_API_KEY 和 FLICKR_API_SECRET")
        print("💡 設定方法：")
        print("   export FLICKR_API_KEY='your_api_key_here'")
        print("   export FLICKR_API_SECRET='your_api_secret_here'")
        print("   🔗 申請 API 金鑰：https://www.flickr.com/services/apps/create/apply")
        exit(1)
    
    print("✅ Flickr API 憑證已載入")
    print()

    # 根據關鍵字數量決定是否使用多線程
    if len(opt.search) > 1:
        # 多個關鍵字，使用多線程
        run_multithread_search(
            search_terms=opt.search,
            n=opt.n,
            download=opt.download,
            size=opt.size,
            max_workers=opt.max_workers
        )
    else:
        # 單個關鍵字，使用單線程
        print("🔄 單線程模式")
        result = get_urls(
            search=opt.search[0], 
            n=opt.n, 
            download=opt.download, 
            size=opt.size,
            thread_id=0
        )
        print(f"\n✅ 完成: {result['count']} 張照片 ({result['elapsed_time']:.1f}s)")
